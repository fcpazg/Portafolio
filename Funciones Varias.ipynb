{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-vehicle",
   "metadata": {},
   "source": [
    "## Hacer Dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expired-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDummies(data, var_name):\n",
    "    '''\n",
    "    This function make the column of a datset in dummies\n",
    "    \n",
    "    In:\n",
    "    DataFrame, \n",
    "    Column Name to be converted.\n",
    "    \n",
    "    Output:\n",
    "    Dataframe with Dummies columns.\n",
    "    '''\n",
    "    \n",
    "    dummy = pd.get_dummies(data[var_name], drop_first=True)\n",
    "    data = data.drop(var_name, axis=1)\n",
    "    data = pd.concat([data, dummy], axis= 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-venice",
   "metadata": {},
   "source": [
    "## Hacer una lista de números "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "satellite-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randint_list (n,a,b):\n",
    "    '''\n",
    "    Function to creat a a random list of n elements between a and b\n",
    "    \n",
    "    In:\n",
    "    n: number of the list elements\n",
    "    a: first number of the interval.\n",
    "    b: second limit of the interval.\n",
    "    \n",
    "    Out:\n",
    "    List\n",
    "    '''\n",
    "    \n",
    "    x=[]\n",
    "    for i in range(n):\n",
    "        x.append(np.random.randint(a,b))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-occasions",
   "metadata": {},
   "source": [
    "## Montecarlo - Aproximaciones de π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "partial-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_MonteCarlo (n_exp, n):\n",
    "    '''\n",
    "    Aproximaciones de π\n",
    "    \n",
    "    In:\n",
    "    n_exp = Número de experimentos\n",
    "    n = Número de puntos generados en cada experimento\n",
    "    \n",
    "    Out:\n",
    "    Valor aproximado de π.\n",
    "    Gráficas con los datos\n",
    "    '''\n",
    "    \n",
    "    #iniciamos el promedio de pi en 0 (que va a ser el resultado que buscamos)\n",
    "    pi_avg = 0    \n",
    "    #hacemos un array vacío para ir ingresando los valores de pi\n",
    "    pi_value_list = []\n",
    "    #Bucle de cada experimento (que se repite n_exp veces)\n",
    "    for i in range (n_exp):\n",
    "        value = 0\n",
    "        # 1.generamos los n valores de x y y y los ponemos en lista:\n",
    "        x= np.random.uniform(0,1,n).tolist()\n",
    "        y= np.random.uniform(0,1,n).tolist()\n",
    "        #2. suma de los cuadrados de x y y\n",
    "        for j in range (n):\n",
    "            z= np.sqrt((x[j]*x[j])+(y[j]*y[j]))\n",
    "            if z<= 1:\n",
    "                value += 1\n",
    "        #convertimos value a float para poder dividirlo.\n",
    "        float_value = float(value)\n",
    "        #sacamos el valor de pi\n",
    "        pi_value = float_value * 4 / n\n",
    "        # agregamnos el valor a la lista:\n",
    "        pi_value_list.append(pi_value)\n",
    "        pi_avg += pi_value\n",
    "\n",
    "    pi= pi_avg/n_exp\n",
    "    fig = plt.plot(pi_value_list)\n",
    "    \n",
    "    return (pi, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-detective",
   "metadata": {},
   "source": [
    "## Obtener coeficiente de correlación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANUAL\n",
    "#Hacemnos una función que saque la correlacion de 2 variables: var1 y var2\n",
    "def corr_coef (df, var1, var2):\n",
    "    \"\"\"\n",
    "    Función para sacar manualmente la correlación de pearson de dos variables.\n",
    "    \n",
    "    IN:\n",
    "    df= DataFrame\n",
    "    var1 = Primera variable (columna)\n",
    "    var2 = Segunda variable (columna)\n",
    "    \n",
    "    Out:\n",
    "    corr_p = Correlción de Pearson\n",
    "    \"\"\"\n",
    "    #sacar el numerador en la correlación:\n",
    "    df[\"corr_num\"] = (df[var1] - np.mean(df[var1])) * (df[var2] - np.mean(df[var2]))\n",
    "    #Para sacar la primera parte del denominador:\n",
    "    df[\"corr_1\"] = (df[var1] - np.mean(df[var1]))**2\n",
    "    #Para sacar la segunda parte del denominador:\n",
    "    df[\"corr_2\"] = (df[var2] - np.mean(df[var2]))**2\n",
    "    #para sacar la correlación de pearson:\n",
    "    corr_p = sum(df[\"corr_num\"]) / np.sqrt(sum(df[\"corr_1\"]) * sum(df[\"corr_2\"]))\n",
    "    \n",
    "    return corr_p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-paste",
   "metadata": {},
   "source": [
    "# Evaluar modelos de Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_regresion(model,x,y, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que evalúa el modelo.\n",
    "    \n",
    "    in:\n",
    "    modelo, x, y, X_train, X_test, y_train, y_test\n",
    "    \n",
    "    out:\n",
    "    RMSE de train\n",
    "    RMSE de test\n",
    "    Gráficas\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    ### CALCULAMOS EL ERROR\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "\n",
    "    ### GRAFICAMOS LOS RESULTADOS\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.scatter(x,y, s = 2, label = 'Datos')\n",
    "    plt.plot(x, y_real, '--',label ='Curva Teórica', c = 'r')\n",
    "    \n",
    "    list1, list2 = zip(*sorted(zip(X_train[:,0], y_train_pred)))\n",
    "    plt.plot(list1, list2,label ='Regresión (train)')\n",
    "    \n",
    "    list1, list2 = zip(*sorted(zip(X_test[:,0], y_test_pred)))\n",
    "    plt.plot(list1, list2,label = 'Regresión (test)')\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 20, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 20, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)\n",
    "\n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
    "    ]\n",
    "\n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-kazakhstan",
   "metadata": {},
   "source": [
    "## Probar diferentes grados de polinomios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba atributos polinómicos x^1, x^2, x^3, x^4 y x^5 con los datos que teníamos de x\n",
    "for idx,potencia_maxima in enumerate(range(1,6)):\n",
    "    print(f'REGRESIÓN CON ATRIBUTOS POLINÓMICOS NUMERO {idx + 1}')\n",
    "    print(f'Agregamos atributos hasta la potencia x^{potencia_maxima}')\n",
    "    \n",
    "    X = x.reshape(-1,1)\n",
    "    for potencia in range(2,potencia_maxima+1):\n",
    "        X = np.hstack((X,(x**potencia).reshape(-1,1)))\n",
    "    print(f'Los atributos tienen forma: {X.shape}')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    ### ENTRENAMOS\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    ### COMPLETAR AQUI PARA RESOLVER CHALLENGE\n",
    "    \n",
    "    evaluar_regresion(reg, x,y, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8b129",
   "metadata": {},
   "source": [
    "# Regla del Codo para KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a hacer una función que haga la regla del codo\n",
    "\n",
    "def regla_codo (data):\n",
    "    \"\"\"\n",
    "    Función pada realizar la regla del codo con KMeans.\n",
    "    \n",
    "    Arguments:\n",
    "    - dataset\n",
    "    \n",
    "    Ouput:\n",
    "    - Gráfico con los codos desde k=1 hasta k=20\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    scores=[]\n",
    "    range_values= range(1,20)\n",
    "    \n",
    "    #Bucle para encontrar el mejor cluster en rango del 1 al 20\n",
    "    for i in range_values:\n",
    "        kmeans = KMeans(n_clusters= i, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        scores.append(kmeans.inertia_)  #WCSS\n",
    "    \n",
    "    #Gráfico:\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range_values, scores, \"bx-\")\n",
    "    plt.title(\"Número óptimo de clústers\")\n",
    "    plt.xlabel(\"Clústers\")\n",
    "    plt.ylabel(\"WCSS(k)\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a9845",
   "metadata": {},
   "source": [
    "## Limpieza de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos una función para remover y limipiar los textos de un dataset:\n",
    "def message_cleaning(message):\n",
    "    \"\"\"\n",
    "    Funcion que limpia textos:\n",
    "    - Quita signos de puntuación \n",
    "    - Quita stopwords\n",
    "    \n",
    "    Arguments:\n",
    "    - Frases o columnas de un dataset\n",
    "    \n",
    "    Output:\n",
    "    - Frases limpias.\n",
    "    \"\"\"\n",
    "    import string\n",
    "    import nltk\n",
    "    \n",
    "    text_punc_removed = [char for char in message if char not in string.punctuation]\n",
    "    text_punc_removed_join = ''.join(text_punc_removed)\n",
    "    text_punc_removed_join_clean = [word for word in text_punc_removed_join.split() if word.lower() not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return text_punc_removed_join_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d816c",
   "metadata": {},
   "source": [
    "## Word Cloud y Frecuencia de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud (data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para hacer nubes de palabras con WorldCloud\n",
    "    \n",
    "    Arguments:\n",
    "    - Columna de dataset\n",
    "    \n",
    "    Output:\n",
    "    - Nube de palabras\n",
    "    - Las 15 palabras que más aparecen en el texto \n",
    "    \"\"\"\n",
    "    \n",
    "    #Importamos librerias:\n",
    "    import nltk\n",
    "    from wordcloud import WordCloud   #para hacer nube de palabras\n",
    "    import re                         #para reemplazar caracteres\n",
    "    stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    #from nltk.stem import WordNetLemmatizer\n",
    "    #lemm = WordNetLemmatizer()\n",
    "    #import itertools\n",
    "    \n",
    "    texto= data.tolist()\n",
    "    \n",
    "    #Reemplzar los caracteres que no sean letras por espacios:\n",
    "    texto= re.sub(\"[^a-zA-ZáéíóúüÁÉÍÓÚÜñÑ]\",\" \",str(texto))\n",
    "    \n",
    "\n",
    "    #Pasar todo a minúsculas\n",
    "    texto= texto.lower()\n",
    "\n",
    "    #Tokenizar para separar las palabras del texto\n",
    "    texto= nltk.word_tokenize(texto)\n",
    "    \n",
    "    #Sacar las Stopwords\n",
    "    texto = [palabra for palabra in texto if not palabra in stopwords]\n",
    "    texto2 = texto.copy()\n",
    "\n",
    "    #Unimos el titular\n",
    "    texto = \" \".join(texto)\n",
    "    \n",
    "    #Para ver las palabras con más frecuencia:\n",
    "    freq = nltk.FreqDist(texto2)\n",
    "    df = pd.DataFrame(freq.items(), columns = [\"Palabra\", \"Frecuencia\"])\n",
    "    df.sort_values(\"Frecuencia\", ascending=False, inplace=True)\n",
    "    df.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    #Hacemnos la nube:\n",
    "    ax= plt.figure(figsize=(20,20))\n",
    "    plt.imshow(WordCloud().generate(str(texto)))\n",
    "    plt.grid(None)\n",
    "    \n",
    "\n",
    "    return ax, df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3fe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee31964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
